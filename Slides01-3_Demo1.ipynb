{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch walkthrough generated by modifying & combining several tutorials:\n",
    "  - https://morvanzhou.github.io/tutorials/\n",
    "  - [Gradient descent and linear regression (stack overflow)](https://stackoverflow.com/questions/17784587/gradient-descent-using-python-and-numpy?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa)\n",
    "\n",
    "This notebeook will walk through:\n",
    "  * numpy/tensors, \n",
    "  * A small regression problem\n",
    "  * Building a neural network using torch.nn\n",
    "  * Regression and classification using NNs\n",
    "\n",
    "Dependencies (tested on):\n",
    "* torch: 0.3.0\n",
    "* matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Numpy and PyTorch variables, tensors, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Linear Regression without PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# m denotes the number of examples\n",
    "def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "    xTrans = x.transpose()\n",
    "    \n",
    "    plt.ion()\n",
    "    \n",
    "    # Run for a certain number of iterations\n",
    "    for i in range(0, numIterations):\n",
    "        \n",
    "        # Run the model forward to get predictions\n",
    "        hypothesis = np.dot(x, theta)\n",
    "        \n",
    "        # Calculate the loss (L1 loss here)\n",
    "        loss = hypothesis - y\n",
    "        \n",
    "        # sum cost per example (the 2 in denominator doesn't really matter here.\n",
    "        # But to be consistent with the gradient, it is included)\n",
    "        cost = np.sum(loss ** 2) / (2*m)\n",
    "                \n",
    "        # Calculate the average gradient per example\n",
    "        gradient = np.dot(xTrans, loss) / m\n",
    "        \n",
    "        # Update the model (parameters) by learning rate and gradient\n",
    "        # Note going in the direction of negative gradient!\n",
    "        theta = theta - alpha * gradient\n",
    "        \n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            # Print out results\n",
    "            print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        \n",
    "        if i % 100000 == 0:\n",
    "            plt.cla()\n",
    "            plt.scatter(x[:,1], y)\n",
    "            #plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "            #plt.text(0.5, 0, 'Loss=%.4f' % loss.data[0], fontdict={'size': 20, 'color':  'red'})\n",
    "            plt.show()\n",
    "            plt.pause(0.2)\n",
    "\n",
    "    plt.ioff()\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation function with some noise\n",
    "def genData(numPoints, slope, bias, variance):\n",
    "    x = np.zeros(shape=(numPoints, 2))\n",
    "    y = np.zeros(shape=numPoints)\n",
    "    \n",
    "    # basically a straight line\n",
    "    for i in range(0, numPoints):\n",
    "        # bias feature\n",
    "        x[i][0] = 1\n",
    "        x[i][1] = i\n",
    "        \n",
    "        # our target variable\n",
    "        y[i] = (i*slope + bias) + random.uniform(0, 1) * variance\n",
    "    return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run data generation and optimization\n",
    "\n",
    "# gen 100 points with a bias of 25 and 10 variance as a bit of noise\n",
    "x, y = genData(100, 0.5, 50, 5)\n",
    "m, n = np.shape(x)\n",
    "#print('Dimensionality of data: X: %d,%d  Y: %d,1') % (m,n,y.shape[0])\n",
    "\n",
    "numIterations= 100000\n",
    "alpha = 0.0001\n",
    "# Test: Change alpha (learning rate) to 0.001 and see what happens!\n",
    "\n",
    "np.random.seed(0)\n",
    "theta = np.random.rand(2)\n",
    "theta = gradientDescent(x, y, theta, alpha, m, numIterations)\n",
    "print(theta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Variables, small functions, and linear regression in PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "x_act = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(200, 1)\n",
    "x_act = Variable(x_act)\n",
    "x_act_np = x_act.data.numpy()   # numpy array for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create activation functions to visualize them\n",
    "y_act_relu = F.relu(x_act).data.numpy()\n",
    "y_act_sigmoid = F.sigmoid(x_act).data.numpy()\n",
    "y_act_tanh = F.tanh(x_act).data.numpy()\n",
    "y_act_softplus = F.softplus(x_act).data.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show them\n",
    "plt.figure(1, figsize=(8, 6))\n",
    "plt.subplot(221)\n",
    "plt.plot(x_act_np, y_act_relu, c='red', label='relu')\n",
    "plt.ylim((-1, 5))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(x_act_np, y_act_sigmoid, c='red', label='sigmoid')\n",
    "plt.ylim((-0.2, 1.2))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(x_act_np, y_act_tanh, c='red', label='tanh')\n",
    "plt.ylim((-1.2, 1.2))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot(x_act_np, y_act_softplus, c='red', label='softplus')\n",
    "plt.ylim((-0.2, 6))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform linear regression via PyTorch\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "# Convert our data from numpy to PyTorch\n",
    "# .float() is required because by default double is returned\n",
    "# unsqueeze is required to add an empty dimension so x_torch is [100,1] not just [100]\n",
    "x_torch = torch.from_numpy(x[:,1]).float().unsqueeze(1)\n",
    "y_torch = torch.from_numpy(y).float().unsqueeze(1)\n",
    "\n",
    "# Create model\n",
    "model = torch.nn.Sequential()\n",
    "model_linear = torch.nn.Linear(1, 1, bias=True)\n",
    "model.add_module(\"linear\", model_linear)\n",
    "\n",
    "# Mean squared error loss\n",
    "loss = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.0)\n",
    "\n",
    "# Determine batch size (how many samples will be used per iteration)\n",
    "batch_size = 100\n",
    "\n",
    "num_iterations = 100000\n",
    "\n",
    "# Run training loop\n",
    "for i in range(num_iterations):\n",
    "    cost = 0.\n",
    "    num_batches = len(x_torch) // batch_size\n",
    "    for k in range(num_batches):\n",
    "        \n",
    "        # Create batch\n",
    "        start, end = k * batch_size, (k + 1) * batch_size\n",
    "        x_batch = x_torch[start:end]\n",
    "        y_batch = y_torch[start:end]\n",
    "        \n",
    "        # Create variables from data\n",
    "        x_var = Variable(x_batch, requires_grad=False)\n",
    "        y_var = Variable(y_batch, requires_grad=False)\n",
    "\n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        fx = model.forward(x_var.view(len(x_var), 1))\n",
    "        output = loss.forward(fx, y_var)\n",
    "\n",
    "        # Backward\n",
    "        output.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += output.data[0]\n",
    "        \n",
    "    \n",
    "    if i % (num_iterations/10) == 0:\n",
    "        print(\"Epoch = %d, cost = %s\" % (i + 1, cost / num_batches))\n",
    "\n",
    "print('\\nLearned parameters:')\n",
    "w = next(model.parameters()).data  # model has only one parameter\n",
    "print(\"Weight = %.2f\" % (w.numpy())) # will be approximately 2\n",
    "\n",
    "print('Bias = %d' % (model_linear.bias))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Building a Neural Network for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many elements that are randomized in neural networks; \n",
    "# it is important to fix the seed to ensure reproducibility\n",
    "torch.manual_seed(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), values from -1 to 1, shape=(100, 1)\n",
    "y_torch = x_torch.pow(2) + 0.2*torch.rand(x_torch.size())                 # noisy y data (tensor), shape=(100, 1)\n",
    "\n",
    "#x_torch = torch.from_numpy(x[:,1]).float().unsqueeze(1)\n",
    "#y_torch = torch.from_numpy(y).float().unsqueeze(1)\n",
    "\n",
    "print(x_torch.size())\n",
    "print(y_torch.size())\n",
    "\n",
    "# torch can only train on Variable, so convert them to Variable\n",
    "x_torch, y_torch = Variable(x_torch), Variable(y_torch)\n",
    "\n",
    "plt.scatter(x_torch.data.numpy(), y_torch.data.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(n_feature=1, n_hidden=10, n_output=1)     # define the network\n",
    "print(net)  # net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()   # something about plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000\n",
    "for t in range(num_iterations):\n",
    "    prediction = net(x_torch)     # input x and predict based on x\n",
    "\n",
    "    loss = loss_func(prediction, y_torch)     # must be (1. nn output, 2. target)\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "\n",
    "    if t % (num_iterations/10) == 0:\n",
    "        print(\"Epoch = %d, loss = %s\" % (t + 1, loss.data.numpy()))\n",
    "        \n",
    "        # plot and show learning process\n",
    "        plt.cla()\n",
    "        plt.scatter(x_torch.data.numpy(), y_torch.data.numpy())\n",
    "        plt.plot(x_torch.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n",
    "        plt.text(0.5, 0, 'Loss=%.4f' % loss.data[0], fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.show()\n",
    "        plt.pause(0.2)\n",
    "\n",
    "plt.ioff()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Building a Neural Network for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make fake data\n",
    "n_data = torch.ones(100, 2)\n",
    "x0 = torch.normal(2*n_data, 1)      # class0 x data (tensor), shape=(100, 2)\n",
    "y0 = torch.zeros(100)               # class0 y data (tensor), shape=(100, 1)\n",
    "x1 = torch.normal(-2*n_data, 1)     # class1 x data (tensor), shape=(100, 2)\n",
    "y1 = torch.ones(100)                # class1 y data (tensor), shape=(100, 1)\n",
    "x = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # shape (200, 2) FloatTensor = 32-bit floating\n",
    "y = torch.cat((y0, y1), ).type(torch.LongTensor)    # shape (200,) LongTensor = 64-bit integer\n",
    "\n",
    "# torch can only train on Variable, so convert them to Variable\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap='spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(n_feature=2, n_hidden=10, n_output=2)     # define the network\n",
    "print(net)  # net architecture\n",
    "\n",
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "loss_func = torch.nn.CrossEntropyLoss()  # the target label is NOT an one-hotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()   # something about plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(100):\n",
    "    out = net(x)                 # input x and predict based on x\n",
    "    loss = loss_func(out, y)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    if t % 10 == 0 or t in [3, 6]:\n",
    "        # plot and show learning process\n",
    "        plt.cla()\n",
    "        _, prediction = torch.max(F.softmax(out), 1)\n",
    "        pred_y = prediction.data.numpy().squeeze()\n",
    "        target_y = y.data.numpy()\n",
    "        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='spring')\n",
    "        accuracy = sum(pred_y == target_y)/200.\n",
    "        plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.show()\n",
    "        plt.pause(0.1)\n",
    "\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#out = net(x_all)\n",
    "#_, prediction = torch.max(F.softmax(out), 1)\n",
    "#pred_y = prediction.data.numpy().squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
